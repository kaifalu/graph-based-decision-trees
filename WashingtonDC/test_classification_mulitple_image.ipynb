{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f109eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Read station_info with attributes\n",
    "station_info = pd.read_csv('C:/Users/Kaifa Lu/UF Courses/URP6931/Data/station_info_attribute_new.csv')\n",
    "station_info['geometry'] = station_info[[\"longitude\", \"latitude\"]].values.tolist()\n",
    "station_info['geometry'] = station_info['geometry'].apply(Point)\n",
    "station_info = gpd.GeoDataFrame(station_info)\n",
    "station_info['Bike_Stop'] = 'Bike_Station' \n",
    "station_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5279067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# station_info[['latitude', 'longitude']].iloc[351]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "\n",
    "from util import config\n",
    "from util.util import colorize\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "\n",
    "def get_parser(ii):\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Semantic Segmentation')\n",
    "    parser.add_argument('--config', type=str, default='config/cityscapes/cityscapes_pspnet101.yaml', help='config file')\n",
    "    parser.add_argument('--image', type=str, default='figure/SVIs_bikesharingstation/SVIs_index_' + str(ii) + '.jpg', help='input image') # ADE_val_00001515\n",
    "    parser.add_argument('opts', help='see config/cityscapes/cityscapes_pspnet101.yaml for all options', default=None, nargs=argparse.REMAINDER)\n",
    "    args_list = ['--config', 'config/cityscapes/cityscapes_pspnet101.yaml', '--image', 'figure/SVIs_bikesharingstation/SVIs_index_' + str(ii) + '.jpg']\n",
    "    args = parser.parse_args(args_list)\n",
    "    assert args.config is not None\n",
    "    cfg = config.load_cfg_from_cfg_file(args.config)\n",
    "    cfg.image = args.image\n",
    "    if args.opts is not None:\n",
    "        cfg = config.merge_cfg_from_list(cfg, args.opts)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    logger_name = \"main-logger\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler()\n",
    "    fmt = \"[%(asctime)s %(levelname)s %(filename)s line %(lineno)d %(process)d] %(message)s\"\n",
    "    handler.setFormatter(logging.Formatter(fmt))\n",
    "    logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def check(args):\n",
    "    assert args.classes > 1\n",
    "    assert args.zoom_factor in [1, 2, 4, 8]\n",
    "    assert args.split in ['train', 'val', 'test']\n",
    "    if args.arch == 'psp':\n",
    "        assert (args.train_h - 1) % 8 == 0 and (args.train_w - 1) % 8 == 0\n",
    "    elif args.arch == 'psa':\n",
    "        if args.compact:\n",
    "            args.mask_h = (args.train_h - 1) // (8 * args.shrink_factor) + 1\n",
    "            args.mask_w = (args.train_w - 1) // (8 * args.shrink_factor) + 1\n",
    "        else:\n",
    "            assert (args.mask_h is None and args.mask_w is None) or (args.mask_h is not None and args.mask_w is not None)\n",
    "            if args.mask_h is None and args.mask_w is None:\n",
    "                args.mask_h = 2 * ((args.train_h - 1) // (8 * args.shrink_factor) + 1) - 1\n",
    "                args.mask_w = 2 * ((args.train_w - 1) // (8 * args.shrink_factor) + 1) - 1\n",
    "            else:\n",
    "                assert (args.mask_h % 2 == 1) and (args.mask_h >= 3) and (\n",
    "                        args.mask_h <= 2 * ((args.train_h - 1) // (8 * args.shrink_factor) + 1) - 1)\n",
    "                assert (args.mask_w % 2 == 1) and (args.mask_w >= 3) and (\n",
    "                        args.mask_w <= 2 * ((args.train_h - 1) // (8 * args.shrink_factor) + 1) - 1)\n",
    "    else:\n",
    "        raise Exception('architecture not supported yet'.format(args.arch))\n",
    "\n",
    "\n",
    "def main(ii):\n",
    "    global args, logger\n",
    "    args = get_parser(ii)\n",
    "    check(args)\n",
    "    logger = get_logger()\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(str(x) for x in args.test_gpu)\n",
    "    logger.info(args)\n",
    "    logger.info(\"=> creating model ...\")\n",
    "    logger.info(\"Classes: {}\".format(args.classes))\n",
    "\n",
    "    value_scale = 255\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    mean = [item * value_scale for item in mean]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    std = [item * value_scale for item in std]\n",
    "    colors = np.loadtxt(args.colors_path).astype('uint8')\n",
    "\n",
    "    if args.arch == 'psp':\n",
    "        from model.pspnet import PSPNet\n",
    "        model = PSPNet(layers=args.layers, classes=args.classes, zoom_factor=args.zoom_factor, pretrained=False)\n",
    "    elif args.arch == 'psa':\n",
    "        from model.psanet import PSANet\n",
    "        model = PSANet(layers=args.layers, classes=args.classes, zoom_factor=args.zoom_factor, compact=args.compact,\n",
    "                       shrink_factor=args.shrink_factor, mask_h=args.mask_h, mask_w=args.mask_w,\n",
    "                       normalization_factor=args.normalization_factor, psa_softmax=args.psa_softmax, pretrained=False)\n",
    "    logger.info(model)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    # cudnn.benchmark = True\n",
    "    if os.path.isfile(args.model_path):\n",
    "        logger.info(\"=> loading checkpoint '{}'\".format(args.model_path))\n",
    "        checkpoint = torch.load(args.model_path, map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        logger.info(\"=> loaded checkpoint '{}'\".format(args.model_path))\n",
    "    else:\n",
    "        raise RuntimeError(\"=> no checkpoint found at '{}'\".format(args.model_path))\n",
    "    return test(model.eval(), args.image, args.classes, mean, std, args.base_size, args.test_h, args.test_w, args.scales, colors)\n",
    "\n",
    "\n",
    "def net_process(model, image, mean, std=None, flip=True):\n",
    "    input = torch.from_numpy(image.transpose((2, 0, 1))).float()\n",
    "    if std is None:\n",
    "        for t, m in zip(input, mean):\n",
    "            t.sub_(m)\n",
    "    else:\n",
    "        for t, m, s in zip(input, mean, std):\n",
    "            t.sub_(m).div_(s)\n",
    "    input = input.unsqueeze(0)\n",
    "    if flip:\n",
    "        input = torch.cat([input, input.flip(3)], 0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input)\n",
    "    _, _, h_i, w_i = input.shape\n",
    "    _, _, h_o, w_o = output.shape\n",
    "    if (h_o != h_i) or (w_o != w_i):\n",
    "        output = F.interpolate(output, (h_i, w_i), mode='bilinear', align_corners=True)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    if flip:\n",
    "        output = (output[0] + output[1].flip(2)) / 2\n",
    "    else:\n",
    "        output = output[0]\n",
    "    output = output.data.cpu().numpy()\n",
    "    output = output.transpose(1, 2, 0)\n",
    "    return output\n",
    "\n",
    "\n",
    "def scale_process(model, image, classes, crop_h, crop_w, h, w, mean, std=None, stride_rate=2/3):\n",
    "    ori_h, ori_w, _ = image.shape\n",
    "    pad_h = max(crop_h - ori_h, 0)\n",
    "    pad_w = max(crop_w - ori_w, 0)\n",
    "    pad_h_half = int(pad_h / 2)\n",
    "    pad_w_half = int(pad_w / 2)\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        image = cv2.copyMakeBorder(image, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=mean)\n",
    "    new_h, new_w, _ = image.shape\n",
    "    stride_h = int(np.ceil(crop_h*stride_rate))\n",
    "    stride_w = int(np.ceil(crop_w*stride_rate))\n",
    "    grid_h = int(np.ceil(float(new_h-crop_h)/stride_h) + 1)\n",
    "    grid_w = int(np.ceil(float(new_w-crop_w)/stride_w) + 1)\n",
    "    prediction_crop = np.zeros((new_h, new_w, classes), dtype=float)\n",
    "    count_crop = np.zeros((new_h, new_w), dtype=float)\n",
    "    for index_h in range(0, grid_h):\n",
    "        for index_w in range(0, grid_w):\n",
    "            s_h = index_h * stride_h\n",
    "            e_h = min(s_h + crop_h, new_h)\n",
    "            s_h = e_h - crop_h\n",
    "            s_w = index_w * stride_w\n",
    "            e_w = min(s_w + crop_w, new_w)\n",
    "            s_w = e_w - crop_w\n",
    "            image_crop = image[s_h:e_h, s_w:e_w].copy()\n",
    "            count_crop[s_h:e_h, s_w:e_w] += 1\n",
    "            prediction_crop[s_h:e_h, s_w:e_w, :] += net_process(model, image_crop, mean, std)\n",
    "    prediction_crop /= np.expand_dims(count_crop, 2)\n",
    "    prediction_crop = prediction_crop[pad_h_half:pad_h_half+ori_h, pad_w_half:pad_w_half+ori_w]\n",
    "    prediction = cv2.resize(prediction_crop, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def test(model, image_path, classes, mean, std, base_size, crop_h, crop_w, scales, colors):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)  # BGR 3 channel ndarray wiht shape H * W * 3\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # convert cv2 read image from BGR order to RGB order\n",
    "    h, w, _ = image.shape\n",
    "    prediction = np.zeros((h, w, classes), dtype=float)\n",
    "    for scale in scales:\n",
    "        long_size = round(scale * base_size)\n",
    "        new_h = long_size\n",
    "        new_w = long_size\n",
    "        if h > w:\n",
    "            new_w = round(long_size/float(h)*w)\n",
    "        else:\n",
    "            new_h = round(long_size/float(w)*h)\n",
    "        image_scale = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        prediction += scale_process(model, image_scale, classes, crop_h, crop_w, h, w, mean, std)\n",
    "    prediction = scale_process(model, image_scale, classes, crop_h, crop_w, h, w, mean, std)\n",
    "    prediction = np.argmax(prediction, axis=2)\n",
    "    gray = np.uint8(prediction)\n",
    "    color = colorize(gray, colors)\n",
    "    image_name = image_path.split('/')[-1].split('.')[0]\n",
    "    gray_path = os.path.join('./figure/SVIs_bikesharingstation/', image_name + '_gray.png')\n",
    "    color_path = os.path.join('./figure/SVIs_bikesharingstation/', image_name + '_color.png')\n",
    "    cv2.imwrite(gray_path, gray)\n",
    "    color.save(color_path)\n",
    "    logger.info(\"=> Prediction saved in {}\".format(color_path))\n",
    "    return prediction\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    visual_quality_station = np.zeros((len(station_info), 19))\n",
    "    for i in range(len(station_info)):\n",
    "        print(i)\n",
    "        prediction = main(i)\n",
    "        length_pixel = len(prediction.reshape(1,-1)[0])\n",
    "        for j in range (19):\n",
    "            visual_quality_station[i, j] = np.sum(prediction.reshape(1,-1)[0] == j)/length_pixel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_pixel = len(prediction.reshape(1,-1)[0])\n",
    "# visual_quality_station = np.zeros((len(station_info), 19))\n",
    "# for i in range (19):\n",
    "#     visual_quality_station[0, i] = np.sum(prediction.reshape(1,-1)[0] == i)/length_pixel\n",
    "# print(visual_quality_station[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd24b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_list = [\n",
    "    'road',\n",
    "    'sidewalk',\n",
    "    'building',\n",
    "    'wall',\n",
    "    'fence',\n",
    "    'pole',\n",
    "    'traffic light',\n",
    "    'traffic sign',\n",
    "    'vegetation',\n",
    "    'terrain',\n",
    "    'sky',\n",
    "    'person',\n",
    "    'rider',\n",
    "    'car',\n",
    "    'truck',\n",
    "    'bus',\n",
    "    'train',\n",
    "    'motorcycle',\n",
    "    'bicycle\n",
    "]\n",
    "station_info_vq = station_info.copy()\n",
    "for kk in range (19):\n",
    "    station_info_vq[column_name_list[kk]] = visual_quality_station[:, kk]\n",
    "station_info_vq.to_csv('station_info_vq.csv', index = False)\n",
    "station_info_vq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce8888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
